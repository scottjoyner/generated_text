#!/usr/bin/env python3
"""
Ingest Hugging Face model JSON (cache/*.json) into Oracle.

Env:
  ORACLE_USER=...
  ORACLE_PASSWORD=...
  ORACLE_DSN=host:port/service_name        # e.g. "localhost:1521/XEPDB1"
    # or use EZCONNECT like "hostname/orclpdb1"

Usage:
  python oracle_ingest.py --cache-dir cache --commit-every 200
"""

import os, json, argparse, pathlib, datetime, sys
from typing import Iterator, Tuple, Any, Optional

# Driver: python-oracledb (thin mode, no client required)
# pip install oracledb
import oracledb

# ---------- Helpers ----------
def split_repo_id(repo_id: str) -> Tuple[str, str]:
    parts = repo_id.split("/", 1)
    return (parts[0], parts[1]) if len(parts) == 2 else ("", repo_id)

def resolve_license(detail_json: dict) -> Optional[str]:
    lic = detail_json.get("license")
    if lic:
        return str(lic)
    card = detail_json.get("cardData") or {}
    card_lic = card.get("license")
    if isinstance(card_lic, str) and card_lic.strip():
        return card_lic.strip()
    if isinstance(card_lic, dict):
        for k in ("name", "id", "spdx"):
            v = card_lic.get(k)
            if isinstance(v, str) and v.strip():
                return v.strip()
    for t in (detail_json.get("tags") or []):
        if isinstance(t, str) and t.lower().startswith("license:"):
            return t.split(":", 1)[1].strip()
    return None

def iso_to_ts(val: Optional[str]) -> Optional[datetime.datetime]:
    if not val:
        return None
    try:
        # Handle trailing 'Z'
        return datetime.datetime.fromisoformat(val.replace("Z", "+00:00"))
    except Exception:
        return None

def iter_cache_json(cache_dir: str) -> Iterator[dict]:
    p = pathlib.Path(cache_dir)
    if not p.exists():
        raise FileNotFoundError(cache_dir)
    for fp in sorted(p.glob("*.json")):
        try:
            yield json.loads(fp.read_text(encoding="utf-8"))
        except Exception as e:
            print(f"[WARN] Skipping {fp.name}: {e}", file=sys.stderr)

# ---------- DDL ----------
DDL = [
    # AUTHORS
    """
    CREATE TABLE authors (
      author_id NUMBER GENERATED BY DEFAULT ON NULL AS IDENTITY,
      name      VARCHAR2(255) NOT NULL,
      CONSTRAINT pk_authors PRIMARY KEY (author_id),
      CONSTRAINT uq_authors_name UNIQUE (name)
    )
    """,
    # MODELS
    """
    CREATE TABLE models (
      id              VARCHAR2(512) NOT NULL,
      author_id       NUMBER NULL,
      name            VARCHAR2(255),
      private         NUMBER(1),
      gated           VARCHAR2(64),
      sha             VARCHAR2(128),
      pipeline_tag    VARCHAR2(128),
      library_name    VARCHAR2(128),
      license         VARCHAR2(128),
      license_raw     VARCHAR2(128),
      downloads       NUMBER,
      likes           NUMBER,
      created_at      TIMESTAMP WITH TIME ZONE,
      last_modified   TIMESTAMP WITH TIME ZONE,
      used_storage    NUMBER(20),
      card_summary    CLOB,
      config_clob         CLOB,
      card_params_clob    CLOB,
      card_model_size_clob CLOB,
      card_datasets_clob  CLOB,
      card_languages_clob CLOB,
      transformers_info_clob CLOB,
      gguf_clob       CLOB,
      safetensors_clob CLOB,
      raw_json        CLOB,
      CONSTRAINT pk_models PRIMARY KEY (id),
      CONSTRAINT fk_models_author FOREIGN KEY (author_id) REFERENCES authors(author_id)
    )
    """,
    # TAGS
    """
    CREATE TABLE tags (
      name VARCHAR2(255) NOT NULL,
      CONSTRAINT pk_tags PRIMARY KEY (name)
    )
    """,
    # MODEL_TAGS
    """
    CREATE TABLE model_tags (
      model_id VARCHAR2(512) NOT NULL,
      tag      VARCHAR2(255) NOT NULL,
      CONSTRAINT pk_model_tags PRIMARY KEY (model_id, tag),
      CONSTRAINT fk_mt_model FOREIGN KEY (model_id) REFERENCES models(id) ON DELETE CASCADE,
      CONSTRAINT fk_mt_tag   FOREIGN KEY (tag)      REFERENCES tags(name)   ON DELETE CASCADE
    )
    """,
    # SIBLINGS (files)
    """
    CREATE TABLE siblings (
      model_id  VARCHAR2(512) NOT NULL,
      rfilename VARCHAR2(512) NOT NULL,
      CONSTRAINT pk_siblings PRIMARY KEY (model_id, rfilename),
      CONSTRAINT fk_sib_model FOREIGN KEY (model_id) REFERENCES models(id) ON DELETE CASCADE
    )
    """,
    # SPACES
    """
    CREATE TABLE spaces (
      name VARCHAR2(512) NOT NULL,
      CONSTRAINT pk_spaces PRIMARY KEY (name)
    )
    """,
    # MODEL_SPACES
    """
    CREATE TABLE model_spaces (
      model_id  VARCHAR2(512) NOT NULL,
      space_name VARCHAR2(512) NOT NULL,
      CONSTRAINT pk_model_spaces PRIMARY KEY (model_id, space_name),
      CONSTRAINT fk_ms_model FOREIGN KEY (model_id) REFERENCES models(id) ON DELETE CASCADE,
      CONSTRAINT fk_ms_space FOREIGN KEY (space_name) REFERENCES spaces(name) ON DELETE CASCADE
    )
    """,
    # Indexes for common filters
    "CREATE INDEX ix_models_author_id    ON models(author_id)",
    "CREATE INDEX ix_models_license      ON models(license)",
    "CREATE INDEX ix_models_pipeline     ON models(pipeline_tag)",
    "CREATE INDEX ix_models_downloads    ON models(downloads)",
    "CREATE INDEX ix_models_likes        ON models(likes)",
    "CREATE INDEX ix_models_lastmodified ON models(last_modified)"
]

def ensure_schema(conn):
    cur = conn.cursor()
    for stmt in DDL:
        try:
            cur.execute(stmt)
            print(f"[SCHEMA] Created object from: {stmt.strip().splitlines()[0][:60]} ...")
        except oracledb.DatabaseError as e:
            # ORA-00955: name is already used by an existing object
            msg = str(e)
            if "ORA-00955" in msg or "ORA-00942" in msg or "already exists" in msg:
                continue
            raise
    cur.close()

# ---------- Upserts ----------
def upsert_author(conn, name: str) -> Optional[int]:
    if not name:
        return None
    cur = conn.cursor()
    # Try select first
    cur.execute("SELECT author_id FROM authors WHERE name=:1", [name])
    row = cur.fetchone()
    if row:
        cur.close()
        return int(row[0])
    # Insert with RETURNING
    try:
        out_id = cur.var(oracledb.NUMBER)
        cur.execute(
            "INSERT INTO authors(name) VALUES (:1) RETURNING author_id INTO :2",
            [name, out_id]
        )
        cur.close()
        return int(out_id.getvalue())
    except oracledb.IntegrityError:
        # Concurrent insert; select again
        cur.execute("SELECT author_id FROM authors WHERE name=:1", [name])
        row = cur.fetchone()
        cur.close()
        return int(row[0]) if row else None

def merge_simple(conn, table: str, key_col: str, key_val: Any):
    """
    Ensure a row exists with primary key/unique 'key_col'='key_val'.
    For tables: TAGS(name), SPACES(name)
    """
    cur = conn.cursor()
    try:
        cur.execute(f"INSERT INTO {table}({key_col}) VALUES (:1)", [key_val])
    except oracledb.IntegrityError:
        pass
    finally:
        cur.close()

def merge_model(conn, rec: dict, author_id: Optional[int]):
    """MERGE for MODELS table (upsert by id)."""
    repo_id = rec.get("id") or rec.get("modelId")
    if not repo_id:
        return
    _, name = split_repo_id(repo_id)
    lic = resolve_license(rec)
    card = rec.get("cardData") or {}

    # Prepare clob text for nested fields
    def dumps(obj):
        return json.dumps(obj, ensure_ascii=False) if obj is not None else None

    sql = """
    MERGE INTO models m
    USING (SELECT :id AS id FROM dual) s
    ON (m.id = s.id)
    WHEN MATCHED THEN UPDATE SET
        author_id       = :author_id,
        name            = :name,
        private         = :private,
        gated           = :gated,
        sha             = :sha,
        pipeline_tag    = :pipeline_tag,
        library_name    = :library_name,
        license         = :license,
        license_raw     = :license_raw,
        downloads       = :downloads,
        likes           = :likes,
        created_at      = :created_at,
        last_modified   = :last_modified,
        used_storage    = :used_storage,
        card_summary    = :card_summary,
        config_clob         = :config_clob,
        card_params_clob    = :card_params_clob,
        card_model_size_clob= :card_model_size_clob,
        card_datasets_clob  = :card_datasets_clob,
        card_languages_clob = :card_languages_clob,
        transformers_info_clob = :transformers_info_clob,
        gguf_clob       = :gguf_clob,
        safetensors_clob = :safetensors_clob,
        raw_json        = :raw_json
    WHEN NOT MATCHED THEN INSERT (
        id, author_id, name, private, gated, sha, pipeline_tag, library_name,
        license, license_raw, downloads, likes, created_at, last_modified, used_storage,
        card_summary, config_clob, card_params_clob, card_model_size_clob,
        card_datasets_clob, card_languages_clob, transformers_info_clob,
        gguf_clob, safetensors_clob, raw_json
    ) VALUES (
        :id, :author_id, :name, :private, :gated, :sha, :pipeline_tag, :library_name,
        :license, :license_raw, :downloads, :likes, :created_at, :last_modified, :used_storage,
        :card_summary, :config_clob, :card_params_clob, :card_model_size_clob,
        :card_datasets_clob, :card_languages_clob, :transformers_info_clob,
        :gguf_clob, :safetensors_clob, :raw_json
    )
    """
    binds = dict(
        id=repo_id,
        author_id=author_id,
        name=name,
        private=1 if rec.get("private") else 0,
        gated=str(rec.get("gated")) if rec.get("gated") is not None else None,
        sha=rec.get("sha"),
        pipeline_tag=rec.get("pipeline_tag"),
        library_name=rec.get("library_name") or card.get("library_name"),
        license=lic,
        license_raw=rec.get("license"),
        downloads=rec.get("downloads"),
        likes=rec.get("likes"),
        created_at=iso_to_ts(rec.get("createdAt")),
        last_modified=iso_to_ts(rec.get("lastModified")),
        used_storage=rec.get("usedStorage"),

        card_summary=card.get("summary") or card.get("description"),
        config_clob=dumps(rec.get("config")),
        card_params_clob=dumps(card.get("params")),
        card_model_size_clob=dumps(card.get("model_size")),
        card_datasets_clob=dumps(card.get("datasets")),
        card_languages_clob=dumps(card.get("language") or card.get("languages")),
        transformers_info_clob=dumps(rec.get("transformersInfo")),
        gguf_clob=dumps(rec.get("gguf")),
        safetensors_clob=dumps(rec.get("safetensors")),
        raw_json=dumps(rec),
    )
    cur = conn.cursor()
    cur.execute(sql, binds)
    cur.close()

def upsert_tags_siblings_spaces(conn, repo_id: str, rec: dict):
    # Tags
    tags = [t for t in (rec.get("tags") or []) if isinstance(t, str)]
    if tags:
        for t in tags:
            merge_simple(conn, "tags", "name", t)
            # MODEL_TAGS via MERGE
            cur = conn.cursor()
            cur.execute("""
                MERGE INTO model_tags mt
                USING (SELECT :model_id AS model_id, :tag AS tag FROM dual) s
                ON (mt.model_id = s.model_id AND mt.tag = s.tag)
                WHEN NOT MATCHED THEN INSERT (model_id, tag) VALUES (s.model_id, s.tag)
            """, {"model_id": repo_id, "tag": t})
            cur.close()
    # Siblings (files)
    siblings = [s.get("rfilename") for s in (rec.get("siblings") or [])
                if isinstance(s, dict) and s.get("rfilename")]
    for rfn in siblings:
        cur = conn.cursor()
        cur.execute("""
            MERGE INTO siblings sb
            USING (SELECT :model_id AS model_id, :rfilename AS rfilename FROM dual) s
            ON (sb.model_id = s.model_id AND sb.rfilename = s.rfilename)
            WHEN NOT MATCHED THEN INSERT (model_id, rfilename) VALUES (s.model_id, s.rfilename)
        """, {"model_id": repo_id, "rfilename": rfn})
        cur.close()
    # Spaces
    spaces = rec.get("spaces") or []
    for sp in spaces:
        merge_simple(conn, "spaces", "name", sp)
        cur = conn.cursor()
        cur.execute("""
            MERGE INTO model_spaces ms
            USING (SELECT :model_id AS model_id, :space AS space_name FROM dual) s
            ON (ms.model_id = s.model_id AND ms.space_name = s.space_name)
            WHEN NOT MATCHED THEN INSERT (model_id, space_name) VALUES (s.model_id, s.space_name)
        """, {"model_id": repo_id, "space": sp})
        cur.close()

# ---------- Main ----------
def main():
    ap = argparse.ArgumentParser(description="Ingest HF cache/*.json into Oracle")
    ap.add_argument("--cache-dir", default="cache")
    ap.add_argument("--commit-every", type=int, default=200)
    args = ap.parse_args()

    user = os.environ.get("ORACLE_USER")
    pwd = os.environ.get("ORACLE_PASSWORD")
    dsn = os.environ.get("ORACLE_DSN")
    if not (user and pwd and dsn):
        print("Set ORACLE_USER, ORACLE_PASSWORD, ORACLE_DSN", file=sys.stderr)
        sys.exit(2)

    conn = oracledb.connect(user=user, password=pwd, dsn=dsn)
    conn.autocommit = False

    ensure_schema(conn)

    count = 0
    try:
        for rec in iter_cache_json(args.cache_dir):
            repo_id = rec.get("id") or rec.get("modelId")
            if not repo_id:
                continue
            author_name, _ = split_repo_id(repo_id)
            author_id = upsert_author(conn, author_name) if author_name else None
            merge_model(conn, rec, author_id)
            upsert_tags_siblings_spaces(conn, repo_id, rec)
            count += 1
            if count % args.commit_every == 0:
                conn.commit()
                print(f"[COMMIT] {count} records")
        conn.commit()
        print(f"[DONE] Ingested {count} records into Oracle.")
    except Exception as e:
        conn.rollback()
        raise
    finally:
        conn.close()

if __name__ == "__main__":
    main()
